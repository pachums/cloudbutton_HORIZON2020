{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>GeoSpatial use case: EX1 High-resolution hybrid land-cover mapping</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objetive of the tutorial is to explain the Experiment 1 of the **GeoSpatial use case** in the H2020 CloudButton project. The main goal of the project is to create a Serverless Data Analytics Platform that aims to “democratize big data” by overly simplifying the overall life cycle and programming model thanks to serverless technologies. The idea is to tap into stateless functions to enable radically-simpler, more user-friendly data processing systems. The CloudButton platform will seamlessly   integrate a Serverless Compute Engine, a Mutable Shared Data Middleware, and new Serverless Cloud Programming Abstractions on top.\n",
    "\n",
    "The first experiment is based in a high-resolution hybrid land-cover mapping through multi-temporal imagery from Sentinel-2. This high resolution raster image is classified with a hybrid land-cover classification based in a supervised classifier, Naive Bayes, and the serverless technology thanks to the framework lithops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Index\n",
    "The tutorial contains the following sections properly explain.\n",
    "\n",
    "**1. Setup**\n",
    "\n",
    "    1.1. Set up docker\n",
    "    \n",
    "    1.2. Importing libraries\n",
    "    \n",
    "    1.3. Downloading Sentinel-2 images\n",
    "    \n",
    "    \n",
    "**2. Dataset processing**\n",
    "\n",
    "    2.1. Merge\n",
    "    \n",
    "    2.2. Mask\n",
    "    \n",
    "    2.3. Tiling\n",
    "    \n",
    "    2.4. Selection\n",
    "    \n",
    "    2.5. Stackering\n",
    "    \n",
    "**3. Machine Learning - Naive Bayes model**\n",
    "\n",
    "    3.1. Load a raster image\n",
    "    \n",
    "    3.2. Obtain the pixels of the training areas\n",
    "    \n",
    "    3.3. Build the training for the classifier\n",
    "    \n",
    "    3.4. Train the model\n",
    "    \n",
    "    3.5. Prediction\n",
    "    \n",
    "    3.6. Export and import trained model \n",
    "    \n",
    "**4. Serverless Implementation - Lithops**\n",
    "\n",
    "    4.1. Configuration lithops\n",
    "    \n",
    "    4.2. Map method\n",
    "    \n",
    "    4.3. Iterdata and execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **first** section will specify all the requeriments and obtain the data from Sentinel-2 that is going to be processed. The **second** part dataset processing, will work with all the satelite images in order to prepare the classification algorithm. A moder is gonna be train and saved for the future use in serverless. In the **third** and last part, all the object from the IBM Cloud are gonna be parallel classified based in the Naive Bayes trained model thanks to the framework lithops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Set up docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with all the libraries in a **jupyter notebook** we will run a **docker** built for the experiment. It is necesarry to download <a href=\"https://www.docker.com/products/docker-desktop\">docker desktop</a>, once it is done, we download and run the image of the docker that its gonna be used.\n",
    "\n",
    "> docker pull mavsonnen/jupy-notebook:geosv2<br>\n",
    "> docker run mavsonnen/jupy-notebook:geosv2\n",
    "\n",
    "A **local directory** can be connected to a docker container creating a volume using <i>-v option</i>, change <i>local_path</i>:\n",
    "\n",
    "> docker run -p 8888:8888 -v local_path:/home/jovyan/work mavsonnen/jupy-notebook:geosv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Importing libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following libraries are necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported succesfully\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from shapely.geometry import mapping\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "from shapely.geometry import mapping\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.plot import show\n",
    "from rasterio.merge import merge\n",
    "from rasterio.plot import show\n",
    "from rasterio.plot import show_hist\n",
    "from rasterio.windows import Window\n",
    "from rasterio.plot import reshape_as_raster\n",
    "from rasterio.plot import reshape_as_image\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lithops\n",
    "import shapely\n",
    "import shutil\n",
    "import pickle\n",
    "import joblib\n",
    "import gdal\n",
    "import time\n",
    "import glob\n",
    "import ogr\n",
    "import os\n",
    "import io\n",
    "\n",
    "print(\"Libraries imported succesfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Downloading Sentinel-2 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with Sentinel-2 images with Level-2A, which already has an atmospheric correction. The data will be downloaded from **Copernicus Open Access Hub** in the following <a href=\"https://scihub.copernicus.eu/dhus/#/home\">link</a>. \n",
    "\n",
    "Copernicus Open Access Hub offers us many searching options like sensing and ingestion period. It is important to work with Sentinel-2 level 2A satelite images, product type has to be set like  <i>S2MSl2A</i>.\n",
    "\n",
    "Using the right click on the map we create the area of interest. As it is showed in the following image, Copernicus will show  all the raster images that overlap the area of interest allowing the user to view the details and choose which ones to **download**.\n",
    "\n",
    "Every product from copernicus comes with a large amount of files and information. In every product it will be needed three **Surface Reflectance** (SRE) bands, the ones which end up in the form: \"SRE_B2\", \"SRE_B3\" and \"SRE_B8\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Yaktocat](https://scihub.copernicus.eu/twiki/pub/SciHubUserGuide/GraphicalUserInterface/gui-10.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Merge\n",
    "We proceed to merge all the raster of the same band. Finally we will have as many raster as bands we are working with, in particular three."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define directories**\n",
    "\n",
    " - <b>input</b>: path to images for the mosaic process\n",
    " \n",
    " - <b>output</b>: output folder where the mosaic image will be saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = '/home/jovyan/work/' \n",
    "\n",
    "input = f\"{tmp}Dataset_valencia/Dataset_sentinel_bands/band3/SENTINEL2*.tif\" \n",
    "output = f\"{tmp}Dataset_valencia/Dataset_by_bands/Mosaic/com_val_band3.tif\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read** all Sentinel2 raster images from input path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jovyan/work/Dataset_valencia/Dataset_sentinel_bands/band3/SENTINEL2A_20200927-110006-031_L2A_T30TYK_C_V2-2_SRE_B3.tif',\n",
       " '/home/jovyan/work/Dataset_valencia/Dataset_sentinel_bands/band3/SENTINEL2A_20200927-110009-082_L2A_T30TXK_C_V2-2_SRE_B3.tif',\n",
       " '/home/jovyan/work/Dataset_valencia/Dataset_sentinel_bands/band3/SENTINEL2B_20200902-105949-025_L2A_T30TYL_C_V2-2_SRE_B3.tif',\n",
       " '/home/jovyan/work/Dataset_valencia/Dataset_sentinel_bands/band3/SENTINEL2B_20200912-110031-737_L2A_T30SYH_C_V2-2_SRE_B3.tif',\n",
       " '/home/jovyan/work/Dataset_valencia/Dataset_sentinel_bands/band3/SENTINEL2B_20201012-110036-590_L2A_T30SXH_C_V2-2_SRE_B3.tif',\n",
       " '/home/jovyan/work/Dataset_valencia/Dataset_sentinel_bands/band3/SENTINEL2B_20201101-110017-667_L2A_T30SYJ_C_V2-2_SRE_B3.tif',\n",
       " '/home/jovyan/work/Dataset_valencia/Dataset_sentinel_bands/band3/SENTINEL2B_20201101-110020-834_L2A_T30SXJ_C_V2-2_SRE_B3.tif']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob(input)\n",
    "filesmosaic=[]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in files:\n",
    "    src = rasterio.open(i)\n",
    "    filesmosaic.append(src)\n",
    "\n",
    "mosaic, out_trans = merge(filesmosaic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy and set **metadata**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_meta = src.meta.copy()\n",
    "out_meta.update({\"driver\": \"GTiff\",\n",
    "                          \"height\": mosaic.shape[1],\n",
    "                          \"width\": mosaic.shape[2],\n",
    "                          \"transform\": out_trans,\n",
    "                          \"crs\": 'epsg:32630'\n",
    "                          }\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save file** in output directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(output, \"w\", **out_meta) as dest:\n",
    "    dest.write(mosaic)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We **repeat** the merge section with **all the bands** we are gonna use for the analyses. In particular we will use three bands B3, B4 and B8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2.2 Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mask the raster images of each band with the administrative delimiter of the region of study. We will perform it with Comunidad Valenciana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define directories**\n",
    "\n",
    " - <b>layer_path</b>: input layer path for masking\n",
    " \n",
    " - <b>raser_path</b>: path of the raster image to be masked\n",
    "  \n",
    " - <b>out_tif</b>: output path to save the mask image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_path = f'{tmp}Dataset_valencia/Dataset_files/comunidad_valenciana.shp'\n",
    "raster_path = f'{tmp}Dataset_valencia/Dataset_by_bands/Mosaic/com_val_band3.tif'\n",
    "out_tif = f'{tmp}Dataset_valencia/Dataset_by_bands/Mask/com_val_band3_mask.tif'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proccess to make high performance image warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutTile = gdal.Warp(out_tif, \n",
    "                    raster_path, \n",
    "                    cutlineDSName=layer_path,\n",
    "                    cropToCutline=True,\n",
    "                    dstNodata = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We **repeat** the mask section with **all the bands** we are gonna use for the analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Tiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create grid\n",
    "\n",
    "Creating a grid for creating smaller raster images out of the bigger raster image.\n",
    "\n",
    "**Define directories**\n",
    "\n",
    " - <b>path_to_layer</b>: path to shp file of the desire extent\n",
    " \n",
    " - <b>output</b>: output folder where the grid will be saved\n",
    " \n",
    " - <b>spacing</b>: spacing between polygons in the grid\n",
    "  \n",
    " - <b>epsg</b>: define epsg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_layer = f'{tmp}Dataset_valencia/Dataset_files/comunidad_valenciana.shp'  \n",
    "output = f'{tmp}Dataset_valencia/Dataset_files/grid_com_val.shp'\n",
    "spacing = 16000\n",
    "epsg = 25830 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define **size of the extent** to be sure the bounding box created has all the set point inside and it is multiple of the spacing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(path_to_layer)\n",
    "xmin, ymin, xmax, ymax = gdf.total_bounds # bounds of the total geometry\n",
    "\n",
    "ytop = np.ceil(np.ceil(ymax) / spacing) * spacing\n",
    "ybottom = np.floor(np.floor(ymin) / spacing) * spacing\n",
    "xright = np.ceil(np.ceil(xmax) / spacing) * spacing\n",
    "xleft = np.floor(np.floor(xmin) / spacing) * spacing\n",
    "\n",
    "# Defining number of rows and columns\n",
    "rows = int((ytop - ybottom) / spacing)\n",
    "cols = int((xright - xleft) / spacing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A for loop for **creating all polygons** of the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons = []\n",
    "it = 0\n",
    "listfid = []\n",
    "\n",
    "for i in np.arange(xleft, xright, spacing):\n",
    "    xleft = i\n",
    "    xright = xleft + spacing\n",
    "    ytop_backup = ytop\n",
    "    for j in np.arange(ytop, ybottom, -spacing):\n",
    "        ytop = j\n",
    "        ybottom = ytop - spacing\n",
    "\n",
    "        polygon = shapely.geometry.Polygon([\n",
    "            (xleft, ytop),\n",
    "            (xright, ytop),\n",
    "            (xright, ybottom),\n",
    "            (xleft, ybottom)\n",
    "        ]\n",
    "        )\n",
    "        polygons.append(polygon)\n",
    "        listfid.append(it)\n",
    "        it += 1\n",
    "    ytop = ytop_backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set metadata and write it into the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pyproj/crs/crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing grid into disk\n"
     ]
    }
   ],
   "source": [
    "srs = f\"epsg:{epsg}\"\n",
    "fid = pd.DataFrame({\"fid_id\": listfid})\n",
    "grid = gpd.GeoDataFrame(fid, geometry=polygons, crs={\"init\": srs})\n",
    "\n",
    "print(\"Writing grid into disk\")\n",
    "grid.to_file(output, driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get layer extent\n",
    "Return the extent of layer with GPKG or ESRI Shapefile formats\n",
    "\n",
    " - <b>layer_path</b>: Path to the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layerextent(layer_path):\n",
    "     # check if it is gpgk or shp file\n",
    "    longitud = len(layer_path.split(\".\"))\n",
    "    driver_name = layer_path.split(\".\")[longitud - 1]\n",
    "    if driver_name == \"gpkg\":\n",
    "        driver = ogr.GetDriverByName(\"GPKG\")\n",
    "    if driver_name == \"shp\":\n",
    "        driver = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
    "        \n",
    "     # open and get the extent\n",
    "    ds = driver.Open(layer_path)\n",
    "    xmin, xmax, ymin, ymax = ds.GetLayer().GetExtent()\n",
    "    extent = f\"{xmin}, {ymin}, {xmax}, {ymax}\"\n",
    "\n",
    "    del ds\n",
    "    raster_extent=extent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer within raster\n",
    "check if a layer is inside the raster\n",
    "\n",
    " - <b>raster_extent</b>: extent of the raster\n",
    " \n",
    " - <b>layer_geom</b>: geometry of the layer\n",
    " \n",
    " - <b>lesser_lextetn</b>: If True a smaller extent is evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_within_raster(raster_extent, layer_geom, lesser_lextent=False):\n",
    "\n",
    "    rxmin, rxmax, rymin, rymax = raster_extent\n",
    "    lxmin, lxmax, lymin, lymax = layer_geom.GetEnvelope()\n",
    "\n",
    "    if lesser_lextent:\n",
    "        # Getting a smaller bounding box\n",
    "        lxmin = lxmin + 100\n",
    "        lxmax = lxmax - 100\n",
    "        lymin = lymin + 100\n",
    "        lymax = lymax - 100\n",
    "\n",
    "    i = 0\n",
    "    if lxmin >= rxmin:  # 1. upper left corner\n",
    "        i += 1\n",
    "    if lymax <= rymax:  # 2. upper right corner\n",
    "        i += 1\n",
    "    if lxmax <= rxmax:  # 3. lower right corner\n",
    "        i += 1\n",
    "    if lymin >= rymin:  # 4. lower left corner\n",
    "        i += 1\n",
    "\n",
    "    if i == 4:\n",
    "        out = True\n",
    "    else:\n",
    "        out = False\n",
    "    return (out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naming convention\n",
    "Creates naming based on the raster name and geometries: date_xmin-ymax_sentineltile_band\n",
    "\n",
    " - <b>raster_path</b>: path to raster file\n",
    " \n",
    " - <b>geometry</b>: geometry of the feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naming_convention(raster_path, geometry):\n",
    "\n",
    "    # xmin, xmax, ymin, ymax\n",
    "    xmin, xmax, ymin, ymax = geometry.GetEnvelope()\n",
    "    splitted = raster_path.split(\"/\")\n",
    "    len_splitted = len(splitted)\n",
    "    name_tmp1 = splitted[len_splitted - 1]\n",
    "    name = name_tmp1.split(\".\")[0]\n",
    "    name_splitted = name.split(\"_\")\n",
    "    if len(name_splitted) < 3:\n",
    "        outaname = f\"{name}_{float(xmin)}-{float(ymax)}\"\n",
    "    else:\n",
    "        sent_tile = name_splitted[0]\n",
    "        band = name_splitted[2]\n",
    "        date_tmp = name_splitted[1]\n",
    "        date = date_tmp.split(\"T\")[0]\n",
    "\n",
    "        outaname = f\"{date}_{float(xmin)}-{float(ymax)}_{sent_tile}_{band}\"\n",
    "    return (outaname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproject\n",
    "Function to reproject an image to a desire espg\n",
    "\n",
    " - <b>image</b>: path to raster image\n",
    " \n",
    " - <b>output_folder</b>: output folder where the output image will be saved\n",
    " \n",
    " - <b>epsg_to</b>: coordinate epsg code to reproject into. 25830 by deafult.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproject(image, output_folder, epsg_to=25830):\n",
    "    \n",
    "    splitted = image.split(\"/\")\n",
    "    lenout = len(splitted)\n",
    "    out_name = splitted[lenout-1]\n",
    "    output = f\"{output_folder}/reprojeted_{out_name}\"\n",
    "\n",
    "    dataset = gdal.Open(image)\n",
    "    srs = osr.SpatialReference()\n",
    "    srs.ImportFromEPSG(epsg_to)\n",
    "    vrt_ds = gdal.AutoCreateWarpedVRT(dataset, None, srs.ExportToWkt(), gdal.GRA_NearestNeighbour)\n",
    "\n",
    "    # cols = vrt_ds.RasterXSize\n",
    "    # rows = vrt_ds.RasterYSize\n",
    "    gdal.GetDriverByName(\"GTiff\").CreateCopy(output, vrt_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking tiles\n",
    "It creates tiles from a raster image based on a grid previously created\n",
    "\n",
    " - <b>layer_tiles</b>: path to grid\n",
    " \n",
    " - <b>raster_path</b>: path to raster\n",
    " \n",
    " - <b>output_folder</b>: path to output folder\n",
    "  \n",
    " - <b>field</b>: Field with cut tiles with\n",
    "  \n",
    " - <b>naming</b>: Apply naming rule (function define before)\n",
    "  \n",
    " - <b>extent</b>: Cut with extent  \n",
    " \n",
    " - <b>lesser_lextent</b>: create an smaller extent \n",
    " \n",
    " - <b>reproyectar</b>: If True, reprojection is applied\n",
    "   \n",
    " - <b>epsg</b>: EPSG code of the srs to reproject into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masking_tiles(layer_tiles,\n",
    "                  raster_path,\n",
    "                  output_folder,\n",
    "                  field=\"fid_id\",\n",
    "                  naming=True,\n",
    "                  extent=False, #Falta desarrollar extent para .tif\n",
    "                  lesser_lextent=False,\n",
    "                  reproyectar=False,\n",
    "                  epsg=25830\n",
    "                  ):\n",
    "    \n",
    "    if os.path.exists(output_folder) is False:\n",
    "        os.mkdir(output_folder)\n",
    "\n",
    "    if reproyectar:\n",
    "        raster_path2 = raster_path\n",
    "        raster_path = reproject(raster_path, output_folder, epsg_to=epsg)\n",
    "        print(raster_path)\n",
    "\n",
    "    driver=ogr.GetDriverByName(\"GPKG\")\n",
    "    ds = driver.Open(layer_tiles)\n",
    "    layer = ds.GetLayer()\n",
    "    i=0\n",
    "    \n",
    "    print('Creating masking tiles files')\n",
    "    for feature in layer: \n",
    "        i=i+1\n",
    "        geom = feature.geometry()\n",
    "        fid = feature.GetField(field)\n",
    "        if naming:\n",
    "            if reproyectar:\n",
    "                out_name = naming_convention(raster_path2, geom)\n",
    "            else:\n",
    "                out_name = naming_convention(raster_path, geom)\n",
    "        else:\n",
    "            out_tmp = raster_path.split(\"/\")\n",
    "            out_tmp2 = out_tmp[len(out_tmp) - 1]\n",
    "            out_name = out_tmp2.split(\".\")[0]\n",
    "\n",
    "        output = f\"{output_folder}/{out_name}.tif\"\n",
    "        print(f'Done successfully: {out_name}')\n",
    "        \n",
    "        if extent:\n",
    "            raster_extent = get_layerextent(raster_path)\n",
    "            #raster_extent = get_rasterextent(raster_path) ORIGINAL\n",
    "            sepuede = layer_within_raster(raster_extent, geom, lesser_lextent=lesser_lextent)\n",
    "\n",
    "            if sepuede:\n",
    "                xmin, xmax, ymin, ymax = geom.GetEnvelope()\n",
    "                lextent = [xmin, ymin, xmax, ymax]\n",
    "\n",
    "                ds2 = gdal.Warp(output,\n",
    "                                raster_path,\n",
    "                                format=\"GTiff\",\n",
    "                                outputBounds=lextent)\n",
    "\n",
    "                del ds2\n",
    "\n",
    "        else:\n",
    "            ds2 = gdal.Warp(output,\n",
    "                            raster_path,\n",
    "                            format=\"GTiff\",\n",
    "                            cutlineDSName=layer_tiles,\n",
    "                            cutlineWhere=f\"{field} = '{fid}'\",\n",
    "                            cropToCutline=True)\n",
    "            del ds2\n",
    "\n",
    "    layer.ResetReading()\n",
    "    ds.FlushCache()\n",
    "\n",
    "    del ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define path to files and execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating masking tiles files\n",
      "Done successfully: val_624000.0-4528000.0_com_band3\n",
      "Done successfully: val_624000.0-4512000.0_com_band3\n",
      "Done successfully: val_624000.0-4496000.0_com_band3\n",
      "Done successfully: val_624000.0-4480000.0_com_band3\n",
      "Done successfully: val_624000.0-4464000.0_com_band3\n",
      "Done successfully: val_624000.0-4448000.0_com_band3\n",
      "Done successfully: val_624000.0-4432000.0_com_band3\n",
      "Done successfully: val_624000.0-4416000.0_com_band3\n",
      "Done successfully: val_624000.0-4400000.0_com_band3\n",
      "Done successfully: val_624000.0-4384000.0_com_band3\n",
      "Done successfully: val_624000.0-4368000.0_com_band3\n",
      "Done successfully: val_624000.0-4352000.0_com_band3\n",
      "Done successfully: val_624000.0-4336000.0_com_band3\n",
      "Done successfully: val_624000.0-4320000.0_com_band3\n",
      "Done successfully: val_624000.0-4304000.0_com_band3\n",
      "Done successfully: val_624000.0-4288000.0_com_band3\n",
      "Done successfully: val_624000.0-4272000.0_com_band3\n",
      "Done successfully: val_624000.0-4256000.0_com_band3\n",
      "Done successfully: val_624000.0-4240000.0_com_band3\n",
      "Done successfully: val_624000.0-4224000.0_com_band3\n",
      "Done successfully: val_624000.0-4208000.0_com_band3\n",
      "Done successfully: val_624000.0-4192000.0_com_band3\n",
      "Done successfully: val_640000.0-4528000.0_com_band3\n",
      "Done successfully: val_640000.0-4512000.0_com_band3\n",
      "Done successfully: val_640000.0-4496000.0_com_band3\n",
      "Done successfully: val_640000.0-4480000.0_com_band3\n",
      "Done successfully: val_640000.0-4464000.0_com_band3\n",
      "Done successfully: val_640000.0-4448000.0_com_band3\n",
      "Done successfully: val_640000.0-4432000.0_com_band3\n",
      "Done successfully: val_640000.0-4416000.0_com_band3\n",
      "Done successfully: val_640000.0-4400000.0_com_band3\n",
      "Done successfully: val_640000.0-4384000.0_com_band3\n",
      "Done successfully: val_640000.0-4368000.0_com_band3\n",
      "Done successfully: val_640000.0-4352000.0_com_band3\n",
      "Done successfully: val_640000.0-4336000.0_com_band3\n",
      "Done successfully: val_640000.0-4320000.0_com_band3\n",
      "Done successfully: val_640000.0-4304000.0_com_band3\n",
      "Done successfully: val_640000.0-4288000.0_com_band3\n",
      "Done successfully: val_640000.0-4272000.0_com_band3\n",
      "Done successfully: val_640000.0-4256000.0_com_band3\n",
      "Done successfully: val_640000.0-4240000.0_com_band3\n",
      "Done successfully: val_640000.0-4224000.0_com_band3\n",
      "Done successfully: val_640000.0-4208000.0_com_band3\n",
      "Done successfully: val_640000.0-4192000.0_com_band3\n",
      "Done successfully: val_656000.0-4528000.0_com_band3\n",
      "Done successfully: val_656000.0-4512000.0_com_band3\n",
      "Done successfully: val_656000.0-4496000.0_com_band3\n",
      "Done successfully: val_656000.0-4480000.0_com_band3\n",
      "Done successfully: val_656000.0-4464000.0_com_band3\n",
      "Done successfully: val_656000.0-4448000.0_com_band3\n",
      "Done successfully: val_656000.0-4432000.0_com_band3\n",
      "Done successfully: val_656000.0-4416000.0_com_band3\n",
      "Done successfully: val_656000.0-4400000.0_com_band3\n",
      "Done successfully: val_656000.0-4384000.0_com_band3\n",
      "Done successfully: val_656000.0-4368000.0_com_band3\n",
      "Done successfully: val_656000.0-4352000.0_com_band3\n",
      "Done successfully: val_656000.0-4336000.0_com_band3\n",
      "Done successfully: val_656000.0-4320000.0_com_band3\n",
      "Done successfully: val_656000.0-4304000.0_com_band3\n",
      "Done successfully: val_656000.0-4288000.0_com_band3\n",
      "Done successfully: val_656000.0-4272000.0_com_band3\n",
      "Done successfully: val_656000.0-4256000.0_com_band3\n",
      "Done successfully: val_656000.0-4240000.0_com_band3\n",
      "Done successfully: val_656000.0-4224000.0_com_band3\n",
      "Done successfully: val_656000.0-4208000.0_com_band3\n",
      "Done successfully: val_656000.0-4192000.0_com_band3\n",
      "Done successfully: val_672000.0-4528000.0_com_band3\n",
      "Done successfully: val_672000.0-4512000.0_com_band3\n",
      "Done successfully: val_672000.0-4496000.0_com_band3\n",
      "Done successfully: val_672000.0-4480000.0_com_band3\n",
      "Done successfully: val_672000.0-4464000.0_com_band3\n",
      "Done successfully: val_672000.0-4448000.0_com_band3\n",
      "Done successfully: val_672000.0-4432000.0_com_band3\n",
      "Done successfully: val_672000.0-4416000.0_com_band3\n",
      "Done successfully: val_672000.0-4400000.0_com_band3\n",
      "Done successfully: val_672000.0-4384000.0_com_band3\n",
      "Done successfully: val_672000.0-4368000.0_com_band3\n",
      "Done successfully: val_672000.0-4352000.0_com_band3\n",
      "Done successfully: val_672000.0-4336000.0_com_band3\n",
      "Done successfully: val_672000.0-4320000.0_com_band3\n",
      "Done successfully: val_672000.0-4304000.0_com_band3\n",
      "Done successfully: val_672000.0-4288000.0_com_band3\n",
      "Done successfully: val_672000.0-4272000.0_com_band3\n",
      "Done successfully: val_672000.0-4256000.0_com_band3\n",
      "Done successfully: val_672000.0-4240000.0_com_band3\n",
      "Done successfully: val_672000.0-4224000.0_com_band3\n",
      "Done successfully: val_672000.0-4208000.0_com_band3\n",
      "Done successfully: val_672000.0-4192000.0_com_band3\n",
      "Done successfully: val_688000.0-4528000.0_com_band3\n",
      "Done successfully: val_688000.0-4512000.0_com_band3\n",
      "Done successfully: val_688000.0-4496000.0_com_band3\n",
      "Done successfully: val_688000.0-4480000.0_com_band3\n",
      "Done successfully: val_688000.0-4464000.0_com_band3\n",
      "Done successfully: val_688000.0-4448000.0_com_band3\n",
      "Done successfully: val_688000.0-4432000.0_com_band3\n",
      "Done successfully: val_688000.0-4416000.0_com_band3\n",
      "Done successfully: val_688000.0-4400000.0_com_band3\n",
      "Done successfully: val_688000.0-4384000.0_com_band3\n",
      "Done successfully: val_688000.0-4368000.0_com_band3\n",
      "Done successfully: val_688000.0-4352000.0_com_band3\n",
      "Done successfully: val_688000.0-4336000.0_com_band3\n",
      "Done successfully: val_688000.0-4320000.0_com_band3\n",
      "Done successfully: val_688000.0-4304000.0_com_band3\n",
      "Done successfully: val_688000.0-4288000.0_com_band3\n",
      "Done successfully: val_688000.0-4272000.0_com_band3\n",
      "Done successfully: val_688000.0-4256000.0_com_band3\n",
      "Done successfully: val_688000.0-4240000.0_com_band3\n",
      "Done successfully: val_688000.0-4224000.0_com_band3\n",
      "Done successfully: val_688000.0-4208000.0_com_band3\n",
      "Done successfully: val_688000.0-4192000.0_com_band3\n",
      "Done successfully: val_704000.0-4528000.0_com_band3\n",
      "Done successfully: val_704000.0-4512000.0_com_band3\n",
      "Done successfully: val_704000.0-4496000.0_com_band3\n",
      "Done successfully: val_704000.0-4480000.0_com_band3\n",
      "Done successfully: val_704000.0-4464000.0_com_band3\n",
      "Done successfully: val_704000.0-4448000.0_com_band3\n",
      "Done successfully: val_704000.0-4432000.0_com_band3\n",
      "Done successfully: val_704000.0-4416000.0_com_band3\n",
      "Done successfully: val_704000.0-4400000.0_com_band3\n",
      "Done successfully: val_704000.0-4384000.0_com_band3\n",
      "Done successfully: val_704000.0-4368000.0_com_band3\n",
      "Done successfully: val_704000.0-4352000.0_com_band3\n",
      "Done successfully: val_704000.0-4336000.0_com_band3\n",
      "Done successfully: val_704000.0-4320000.0_com_band3\n",
      "Done successfully: val_704000.0-4304000.0_com_band3\n",
      "Done successfully: val_704000.0-4288000.0_com_band3\n",
      "Done successfully: val_704000.0-4272000.0_com_band3\n",
      "Done successfully: val_704000.0-4256000.0_com_band3\n",
      "Done successfully: val_704000.0-4240000.0_com_band3\n",
      "Done successfully: val_704000.0-4224000.0_com_band3\n",
      "Done successfully: val_704000.0-4208000.0_com_band3\n",
      "Done successfully: val_704000.0-4192000.0_com_band3\n",
      "Done successfully: val_720000.0-4528000.0_com_band3\n",
      "Done successfully: val_720000.0-4512000.0_com_band3\n",
      "Done successfully: val_720000.0-4496000.0_com_band3\n",
      "Done successfully: val_720000.0-4480000.0_com_band3\n",
      "Done successfully: val_720000.0-4464000.0_com_band3\n",
      "Done successfully: val_720000.0-4448000.0_com_band3\n",
      "Done successfully: val_720000.0-4432000.0_com_band3\n",
      "Done successfully: val_720000.0-4416000.0_com_band3\n",
      "Done successfully: val_720000.0-4400000.0_com_band3\n",
      "Done successfully: val_720000.0-4384000.0_com_band3\n",
      "Done successfully: val_720000.0-4368000.0_com_band3\n",
      "Done successfully: val_720000.0-4352000.0_com_band3\n",
      "Done successfully: val_720000.0-4336000.0_com_band3\n",
      "Done successfully: val_720000.0-4320000.0_com_band3\n",
      "Done successfully: val_720000.0-4304000.0_com_band3\n",
      "Done successfully: val_720000.0-4288000.0_com_band3\n",
      "Done successfully: val_720000.0-4272000.0_com_band3\n",
      "Done successfully: val_720000.0-4256000.0_com_band3\n",
      "Done successfully: val_720000.0-4240000.0_com_band3\n",
      "Done successfully: val_720000.0-4224000.0_com_band3\n",
      "Done successfully: val_720000.0-4208000.0_com_band3\n",
      "Done successfully: val_720000.0-4192000.0_com_band3\n",
      "Done successfully: val_736000.0-4528000.0_com_band3\n",
      "Done successfully: val_736000.0-4512000.0_com_band3\n",
      "Done successfully: val_736000.0-4496000.0_com_band3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done successfully: val_736000.0-4480000.0_com_band3\n",
      "Done successfully: val_736000.0-4464000.0_com_band3\n",
      "Done successfully: val_736000.0-4448000.0_com_band3\n",
      "Done successfully: val_736000.0-4432000.0_com_band3\n",
      "Done successfully: val_736000.0-4416000.0_com_band3\n",
      "Done successfully: val_736000.0-4400000.0_com_band3\n",
      "Done successfully: val_736000.0-4384000.0_com_band3\n",
      "Done successfully: val_736000.0-4368000.0_com_band3\n",
      "Done successfully: val_736000.0-4352000.0_com_band3\n",
      "Done successfully: val_736000.0-4336000.0_com_band3\n",
      "Done successfully: val_736000.0-4320000.0_com_band3\n",
      "Done successfully: val_736000.0-4304000.0_com_band3\n",
      "Done successfully: val_736000.0-4288000.0_com_band3\n",
      "Done successfully: val_736000.0-4272000.0_com_band3\n",
      "Done successfully: val_736000.0-4256000.0_com_band3\n",
      "Done successfully: val_736000.0-4240000.0_com_band3\n",
      "Done successfully: val_736000.0-4224000.0_com_band3\n",
      "Done successfully: val_736000.0-4208000.0_com_band3\n",
      "Done successfully: val_736000.0-4192000.0_com_band3\n",
      "Done successfully: val_752000.0-4528000.0_com_band3\n",
      "Done successfully: val_752000.0-4512000.0_com_band3\n",
      "Done successfully: val_752000.0-4496000.0_com_band3\n",
      "Done successfully: val_752000.0-4480000.0_com_band3\n",
      "Done successfully: val_752000.0-4464000.0_com_band3\n",
      "Done successfully: val_752000.0-4448000.0_com_band3\n",
      "Done successfully: val_752000.0-4432000.0_com_band3\n",
      "Done successfully: val_752000.0-4416000.0_com_band3\n",
      "Done successfully: val_752000.0-4400000.0_com_band3\n",
      "Done successfully: val_752000.0-4384000.0_com_band3\n",
      "Done successfully: val_752000.0-4368000.0_com_band3\n",
      "Done successfully: val_752000.0-4352000.0_com_band3\n",
      "Done successfully: val_752000.0-4336000.0_com_band3\n",
      "Done successfully: val_752000.0-4320000.0_com_band3\n",
      "Done successfully: val_752000.0-4304000.0_com_band3\n",
      "Done successfully: val_752000.0-4288000.0_com_band3\n",
      "Done successfully: val_752000.0-4272000.0_com_band3\n",
      "Done successfully: val_752000.0-4256000.0_com_band3\n",
      "Done successfully: val_752000.0-4240000.0_com_band3\n",
      "Done successfully: val_752000.0-4224000.0_com_band3\n",
      "Done successfully: val_752000.0-4208000.0_com_band3\n",
      "Done successfully: val_752000.0-4192000.0_com_band3\n",
      "Done successfully: val_768000.0-4528000.0_com_band3\n",
      "Done successfully: val_768000.0-4512000.0_com_band3\n",
      "Done successfully: val_768000.0-4496000.0_com_band3\n",
      "Done successfully: val_768000.0-4480000.0_com_band3\n",
      "Done successfully: val_768000.0-4464000.0_com_band3\n",
      "Done successfully: val_768000.0-4448000.0_com_band3\n",
      "Done successfully: val_768000.0-4432000.0_com_band3\n",
      "Done successfully: val_768000.0-4416000.0_com_band3\n",
      "Done successfully: val_768000.0-4400000.0_com_band3\n",
      "Done successfully: val_768000.0-4384000.0_com_band3\n",
      "Done successfully: val_768000.0-4368000.0_com_band3\n",
      "Done successfully: val_768000.0-4352000.0_com_band3\n",
      "Done successfully: val_768000.0-4336000.0_com_band3\n",
      "Done successfully: val_768000.0-4320000.0_com_band3\n",
      "Done successfully: val_768000.0-4304000.0_com_band3\n",
      "Done successfully: val_768000.0-4288000.0_com_band3\n",
      "Done successfully: val_768000.0-4272000.0_com_band3\n",
      "Done successfully: val_768000.0-4256000.0_com_band3\n",
      "Done successfully: val_768000.0-4240000.0_com_band3\n",
      "Done successfully: val_768000.0-4224000.0_com_band3\n",
      "Done successfully: val_768000.0-4208000.0_com_band3\n",
      "Done successfully: val_768000.0-4192000.0_com_band3\n",
      "Done successfully: val_784000.0-4528000.0_com_band3\n",
      "Done successfully: val_784000.0-4512000.0_com_band3\n",
      "Done successfully: val_784000.0-4496000.0_com_band3\n",
      "Done successfully: val_784000.0-4480000.0_com_band3\n",
      "Done successfully: val_784000.0-4464000.0_com_band3\n",
      "Done successfully: val_784000.0-4448000.0_com_band3\n",
      "Done successfully: val_784000.0-4432000.0_com_band3\n",
      "Done successfully: val_784000.0-4416000.0_com_band3\n",
      "Done successfully: val_784000.0-4400000.0_com_band3\n",
      "Done successfully: val_784000.0-4384000.0_com_band3\n",
      "Done successfully: val_784000.0-4368000.0_com_band3\n",
      "Done successfully: val_784000.0-4352000.0_com_band3\n",
      "Done successfully: val_784000.0-4336000.0_com_band3\n",
      "Done successfully: val_784000.0-4320000.0_com_band3\n",
      "Done successfully: val_784000.0-4304000.0_com_band3\n",
      "Done successfully: val_784000.0-4288000.0_com_band3\n",
      "Done successfully: val_784000.0-4272000.0_com_band3\n",
      "Done successfully: val_784000.0-4256000.0_com_band3\n",
      "Done successfully: val_784000.0-4240000.0_com_band3\n",
      "Done successfully: val_784000.0-4224000.0_com_band3\n",
      "Done successfully: val_784000.0-4208000.0_com_band3\n",
      "Done successfully: val_784000.0-4192000.0_com_band3\n",
      "Done successfully: val_800000.0-4528000.0_com_band3\n",
      "Done successfully: val_800000.0-4512000.0_com_band3\n",
      "Done successfully: val_800000.0-4496000.0_com_band3\n",
      "Done successfully: val_800000.0-4480000.0_com_band3\n",
      "Done successfully: val_800000.0-4464000.0_com_band3\n",
      "Done successfully: val_800000.0-4448000.0_com_band3\n",
      "Done successfully: val_800000.0-4432000.0_com_band3\n",
      "Done successfully: val_800000.0-4416000.0_com_band3\n",
      "Done successfully: val_800000.0-4400000.0_com_band3\n",
      "Done successfully: val_800000.0-4384000.0_com_band3\n",
      "Done successfully: val_800000.0-4368000.0_com_band3\n",
      "Done successfully: val_800000.0-4352000.0_com_band3\n",
      "Done successfully: val_800000.0-4336000.0_com_band3\n",
      "Done successfully: val_800000.0-4320000.0_com_band3\n",
      "Done successfully: val_800000.0-4304000.0_com_band3\n",
      "Done successfully: val_800000.0-4288000.0_com_band3\n",
      "Done successfully: val_800000.0-4272000.0_com_band3\n",
      "Done successfully: val_800000.0-4256000.0_com_band3\n",
      "Done successfully: val_800000.0-4240000.0_com_band3\n",
      "Done successfully: val_800000.0-4224000.0_com_band3\n",
      "Done successfully: val_800000.0-4208000.0_com_band3\n",
      "Done successfully: val_800000.0-4192000.0_com_band3\n"
     ]
    }
   ],
   "source": [
    "layer_tiles = f'{tmp}Dataset_valencia/Dataset_files/grid_com_val.gpkg'\n",
    "raster_path = f'{tmp}Dataset_valencia/Dataset_by_bands/Mask/com_val_band3_mask.tif'\n",
    "output_folder = f'{tmp}Dataset_valencia/Dataset_by_bands/Tiling_b3'\n",
    "\n",
    "masking_tiles(layer_tiles, raster_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define directories**\n",
    "\n",
    " - <b>input_path</b>: path where all the tiling raster images are located\n",
    " \n",
    " - <b>output_path</b>: path for the selection of the raster images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = f'{tmp}Dataset_valencia/Dataset_by_bands/Tiling_b8/'\n",
    "output_path = f'{tmp}Dataset_valencia/Dataset_by_bands/Tiling_b8_selection/'\n",
    "\n",
    "criteria = 'valenciana*.tif'\n",
    "all_files = glob.glob(input_path+criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop checking which raster images are merely water and discarding them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the file: valenciana_624000.0-4368000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_624000.0-4384000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_624000.0-4400000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_624000.0-4432000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_624000.0-4448000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_640000.0-4320000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_640000.0-4336000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_640000.0-4352000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_640000.0-4368000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_640000.0-4384000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_640000.0-4400000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_640000.0-4416000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_640000.0-4432000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_640000.0-4448000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_640000.0-4464000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_656000.0-4256000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_656000.0-4272000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_656000.0-4288000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_656000.0-4320000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_656000.0-4336000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_656000.0-4352000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_656000.0-4368000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_656000.0-4384000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_656000.0-4400000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_656000.0-4416000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_656000.0-4432000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_656000.0-4448000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_672000.0-4208000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_672000.0-4224000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_672000.0-4240000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_672000.0-4256000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_672000.0-4272000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_672000.0-4288000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_672000.0-4304000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_672000.0-4320000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_672000.0-4336000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_672000.0-4352000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_672000.0-4368000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_672000.0-4384000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_672000.0-4400000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_672000.0-4416000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_672000.0-4432000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_688000.0-4192000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_688000.0-4208000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_688000.0-4224000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_688000.0-4240000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_688000.0-4256000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_688000.0-4272000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_688000.0-4288000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_688000.0-4304000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_688000.0-4320000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_688000.0-4336000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_688000.0-4352000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_688000.0-4368000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_688000.0-4384000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_688000.0-4400000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_688000.0-4416000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_688000.0-4432000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_688000.0-4448000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_704000.0-4208000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_704000.0-4224000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_704000.0-4240000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_704000.0-4256000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_704000.0-4272000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_704000.0-4288000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_704000.0-4304000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_704000.0-4320000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_704000.0-4336000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_704000.0-4352000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_704000.0-4368000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_704000.0-4384000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_704000.0-4400000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_704000.0-4416000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_704000.0-4432000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_704000.0-4448000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_704000.0-4464000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_720000.0-4240000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_720000.0-4256000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_720000.0-4272000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_720000.0-4288000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_720000.0-4304000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_720000.0-4320000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_720000.0-4336000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_720000.0-4352000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_720000.0-4368000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_720000.0-4384000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_720000.0-4400000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_720000.0-4416000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_720000.0-4432000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_720000.0-4448000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_720000.0-4464000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_720000.0-4480000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_720000.0-4496000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_720000.0-4512000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_720000.0-4528000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_736000.0-4272000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_736000.0-4288000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_736000.0-4304000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_736000.0-4320000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_736000.0-4336000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_736000.0-4352000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_736000.0-4400000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_736000.0-4416000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_736000.0-4432000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_736000.0-4448000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_736000.0-4464000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_736000.0-4480000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_736000.0-4496000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_736000.0-4512000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_736000.0-4528000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_752000.0-4272000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_752000.0-4288000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_752000.0-4304000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_752000.0-4320000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_752000.0-4432000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_752000.0-4448000.0_comunidad_band8.tif\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the file: valenciana_752000.0-4464000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_752000.0-4480000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_752000.0-4496000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_752000.0-4512000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_752000.0-4528000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_768000.0-4288000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_768000.0-4304000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_768000.0-4320000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_768000.0-4448000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_768000.0-4464000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_768000.0-4480000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_768000.0-4496000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_768000.0-4512000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_768000.0-4528000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_784000.0-4480000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_784000.0-4496000.0_comunidad_band8.tif\n",
      "Processing the file: valenciana_784000.0-4512000.0_comunidad_band8.tif\n"
     ]
    }
   ],
   "source": [
    "for raster_path in all_files:\n",
    "    raster = rasterio.open(raster_path)\n",
    "    data_matrix = raster.read()\n",
    "\n",
    "    file_name = raster_path.split('/')[len(raster_path.split('/'))-1]\n",
    "\n",
    "    if np.unique(data_matrix).any()!=0:\n",
    "        print(f'Processing the file: {file_name}')\n",
    "        tmp = rasterio.open(output_path + file_name, 'w', driver='Gtiff',\n",
    "                                 width=raster.width, height=raster.height,\n",
    "                                 count=raster.count,\n",
    "                                 crs=raster.crs,\n",
    "                                 transform=raster.transform,\n",
    "                                 dtype=raster.dtypes[0]\n",
    "                                 )\n",
    "        tmp.write(raster.read(1),1)\n",
    "        \n",
    "        if raster.count != 1:\n",
    "            tmp.write(raster.read(2),2)\n",
    "            tmp.write(raster.read(3),3)\n",
    "            \n",
    "        tmp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Stackering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this section is to **stack all the selected bands** in a unique raster image for the future classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define que path where the previous tiling raster images are store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_b3 = f'{tmp}Dataset_valencia/Dataset_by_bands/Tiling_b3_selection'\n",
    "files_b4 = f'{tmp}Dataset_valencia/Dataset_by_bands/Tiling_b4_selection'\n",
    "files_b8 = f'{tmp}Dataset_valencia/Dataset_by_bands/Tiling_b8_selection'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the criteria we select all the bands for the stackering process and get all the coordinates for the name convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = '/valenciana*.tif'\n",
    "all_name_file=[]\n",
    "all_files=[]\n",
    "\n",
    "for name_band in [files_b3,files_b4,files_b8]:\n",
    "    input_directory = name_band + criteria\n",
    "    list_files = glob.glob(input_directory)\n",
    "    all_files.append(list_files)\n",
    "\n",
    "for x in list_files:   \n",
    "    name_file = x.split(\"/\")[len(x.split(\"/\"))-1].split(\"_\")[1]\n",
    "    all_name_file.append(name_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the **name convetion** we create all the folder to store the bands in each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in all_name_file:\n",
    "    try:\n",
    "        os.mkdir(f\"{tmp}/Dataset_valencia/Dataset_by_bands/Stackering/{name}\")\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We **move bands** to each folder that was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(all_files[0])):\n",
    "    shutil.move(all_files[0][i],f\"{tmp}/Dataset_valencia/Dataset_by_bands/Stackering/{all_name_file[i]}\")\n",
    "    shutil.move(all_files[1][i],f\"{tmp}/Dataset_valencia/Dataset_by_bands/Stackering/{all_name_file[i]}\")\n",
    "    shutil.move(all_files[2][i],f\"{tmp}/Dataset_valencia/Dataset_by_bands/Stackering/{all_name_file[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the **metadata information** from one band and update it for later define it over the stacked raster image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: \"<closed DatasetWriter name='/home/jovyan/work/Dataset_valencia/Dataset_by_bands/Tiling_b8_selection/valenciana_784000.0-4512000.0_comunidad_band8.tif' mode='w'>/Dataset_valencia/Dataset_by_bands/Stackering/\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-4ee4a86a37bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{tmp}/Dataset_valencia/Dataset_by_bands/Stackering/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcoord_bands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{tmp}/Dataset_valencia/Dataset_by_bands/Stackering/{list[i]}/val*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#print(coord_bands)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: \"<closed DatasetWriter name='/home/jovyan/work/Dataset_valencia/Dataset_by_bands/Tiling_b8_selection/valenciana_784000.0-4512000.0_comunidad_band8.tif' mode='w'>/Dataset_valencia/Dataset_by_bands/Stackering/\""
     ]
    }
   ],
   "source": [
    "list = os.listdir(f\"{tmp}/Dataset_valencia/Dataset_by_bands/Stackering/\")\n",
    "\n",
    "for i in range(0,len(list)):\n",
    "    coord_bands = glob.glob(f\"{tmp}/Dataset_valencia/Dataset_by_bands/Stackering/{list[i]}/val*\")\n",
    "    #print(coord_bands)\n",
    "    \n",
    "    raster_meta = rasterio.open(coord_bands[0])\n",
    "    meta = raster_meta.meta \n",
    "    meta.update(count = len(coord_bands))\n",
    "\n",
    "    img_stack = f\"{tmp}/Dataset_valencia/Dataset_by_bands/Stackering_output/{list[i]}.tif\"\n",
    "    \n",
    "    with rasterio.open(img_stack, 'w', **meta) as dst:\n",
    "        for id, layer in enumerate(coord_bands, start=1):\n",
    "            with rasterio.open(layer) as src1:\n",
    "                dst.write_band(id, src1.read(1))\n",
    "    print(f'Stackering process finished, raster image {list[i]} created.')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We **verify** the raster image has been properly built after the stackering process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = rasterio.open(img_stack)\n",
    "print('The stakering image has the shape: {} and it is composed of {} bands.'.format(full_data.shape,full_data.count))\n",
    "show(full_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Machine Learning - Naive Bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Obtain the pixels of the training areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raster is gonna be splitted in different areas for the future training of the classifier. For this step we need to **create an ERSI Shapefile** with different training polygons, the ones that are gonna be used for the training process. We can help from QGIS or SNAP software for creation of those areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://community.hexagongeospatial.com/t5/image/serverpage/image-id/4240iF1A4D500227ABC21/image-size/original?v=1.0&px=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the **directory** to the training areas shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'valenciana_704000.0-4400000.0_comunidad_mask'\n",
    "train_areas = gpd.read_file(f'{tmp}training/areas_{name}.shp')\n",
    "print('The training area has the shape: {} with a coordinate reference system {}.'.format(train_areas.shape,train_areas.crs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Note</u>: if it does not have the same coordinate system of the dataset use to_crs and set the correct epsg. Now we generate a list of geometries and check a random feature to verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoms = train_areas.geometry.values\n",
    "geometry = geoms[0]\n",
    "geoms[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **raster values are extracted** within the training areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_stack = tmp + 'tiling/' + name +'.tif'\n",
    "full_data = rasterio.open(img_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = [mapping(geometry)]\n",
    "out_img_extr, out_transform = mask(full_data, feature, crop=True)\n",
    "out_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Build the training for the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previous data we have been analysing, we create two arrays with the **feature and labels data** for the training of the classifier. First we define two empty arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([], dtype=np.int8).reshape(0,3) # pixels\n",
    "y = np.array([], dtype=np.string_) # labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract the value** of our raster within the training polygons. We get <i>X</i> and <i>y</i>, the features and layers arrays respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(img_stack) as src:\n",
    "    band_count = src.count\n",
    "    \n",
    "    for index, geom in enumerate(geoms):      \n",
    "        feature = [mapping(geom)] # transform to gjson format for an easier manipulation.      \n",
    "        out_img_extr, out_transform = mask(src, feature, crop=True) # mask return pixel in to array    \n",
    "        out_img_extr_trimmed = out_img_extr[:,~np.all(out_img_extr == 0, axis=0)] # remove all pixels with 0 values \n",
    "        out_img_extr_trimmed = out_img_extr_trimmed[:,~np.all(out_img_extr_trimmed == 255, axis=0)] # remove all pixels with 255 values  \n",
    "        out_img_extr_reshaped = out_img_extr_trimmed.reshape(-1, band_count) # reshape the array\n",
    "        \n",
    "        y = np.append(y,[train_areas[\"layer\"][index]] * out_img_extr_reshaped.shape[0]) # add the labels to array\n",
    "        X = np.vstack((X,out_img_extr_reshaped)) # stack the pixels to array\n",
    "\n",
    "print('We define X as the array that stores the features with a shape {} and y as the labels array with a shape {}.'.format(X.shape,y.shape))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the different classes** that are define in our raster image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The raster image contain {} classes which are the following: \\n {}'.format(np.unique(train_areas[\"layer\"]).size, np.unique(train_areas[\"layer\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.Train the model\n",
    "\n",
    "\n",
    "Once all the data have been proccess we are ready to **train the classifier**, in this section the classifier will learn from our data in order to make an accurate classification for the next raster image to be classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X, y)\n",
    "print(\"Naive Bayes trainned.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to achieve a better classification the model have to be **train with more raster images**. The dataset used to train the model have to be diverse and paying attention not to fall in overfiting or underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed to make the prediction, however it is gonna be done with serverless technology in the following sections so migth skip this point 2.5.\n",
    "\n",
    "We advance to the **prediction of the raster**. Based on what the classifier has learn it will make a classification with a maximun of 10 different classes, the ones are shown bellow.\n",
    "\n",
    "- artificial surface\n",
    "- water surface\n",
    "- transition vegetation\n",
    "- conifers\n",
    "- hardwood forest\n",
    "- natural grasslands\n",
    "- mediterranean scrub\n",
    "- dry farming land\n",
    "- agricultural areas\n",
    "- wetlands\n",
    "- burned areas\n",
    "\n",
    "It is defined a function <i>str_class_to_int</i> that assigns the classes to indices and some modification of the raster shape to be able to perform the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_class_to_int(class_array):\n",
    "    class_array[class_array == 'superficie_artificial'] = 1\n",
    "    class_array[class_array == 'bosque_de_coniferas'] = 2\n",
    "    class_array[class_array == 'bosque_de_frondosas'] = 3\n",
    "    class_array[class_array == 'zonas_agricolas'] = 4\n",
    "    class_array[class_array == 'vegetacion_esclerofila'] = 5\n",
    "    class_array[class_array == 'pastizales_naturales'] = 6\n",
    "    class_array[class_array == 'matorral_boscoso_de_transicion'] = 7\n",
    "    class_array[class_array == 'zonas_quedamas'] = 8\n",
    "    class_array[class_array == 'zonas_humedas'] = 9\n",
    "    class_array[class_array == 'superficie_agua'] = 10\n",
    "    return(class_array.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the computing cost is making the process significantly slower, if it is necessary to work with a part of the image, in that case don't read the whole image defining the **position**: col_off, row_off, width, height. For <u>example</u>: <i>src.read()[:, 1500: 2000, 1500 : 2500]</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(img_stack) as src:\n",
    "    img = src.read()[:, : , :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reshape** the complete image and in a long 2d matrix to be able to visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_img = reshape_as_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each pixel of our we **perform a prediction** and reshape our classified image in a 2d matrix to be able to see it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = gnb.predict(reshaped_img.reshape(-1, 3))\n",
    "prediction = prediction.reshape(reshaped_img[:, :, 0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the shapefile initially convert the **strings to numpy matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = str_class_to_int(prediction)\n",
    "print(np.unique(prediction))\n",
    "print(prediction.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show prediction** to have an overview of the raster classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(prediction, cmap='terrain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can **save** the array as a raster image .tif which is the classified image. First we change the format to <i>int32</i> and then create an output raster image from the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = prediction.astype(np.int32) \n",
    "\n",
    "SalidaR = rasterio.open(tmp + 'out3.tif','w',\n",
    "                   driver='Gtiff',\n",
    "                   width = 1599,\n",
    "                   height = 1599,\n",
    "                   count=1,\n",
    "                   crs = full_data.crs,  \n",
    "                   transform = full_data.transform, \n",
    "                   dtype='int32')\n",
    "SalidaR.write(y,1)\n",
    "SalidaR.close()\n",
    "\n",
    "print(\"Raster image succesfully created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Export and import  trained model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the **Machine Learning model** based in Naive Bayes through <i>joblib</i> and import it. By doing so we will be able to upload it to IBM_COS. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gnb\n",
    "name_model = 'naive_bayes_model.sav' \n",
    "joblib.dump(model, tmp + '/COS/' + name_model)\n",
    "\n",
    "print(\"Trained model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load(tmp + '/COS/' + name_model)\n",
    "\n",
    "print(\"Train model imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Serverless Implementation - Lithops\n",
    "\n",
    "We have reached the serverless implementation thanks to the framework **lithops**. The model which was trained before will be used to classify different raster images in order to extract the different classes.\n",
    "\n",
    "All files that are gonna be used for the classification, raster images and models, require to be uploaded in **IBM Cloud Object Storage**. The raster images are need to be storage in the same bucket for the parallel classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Yaktocat](https://github.com/lithops-cloud/lithops/blob/master/docs/images/lithops_flat_cloud_1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Configuration of lithops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with lithops require a configuration file like the one express below. More info can be found in the official **<a href=\"https://github.com/lithops-cloud/lithops\">github</a> of Lithops**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <i>\n",
    "    config = { 'lithops' : {'storage_bucket' : 'BUCKET_NAME'},\n",
    "    \n",
    "           'ibm_cf':  {'endpoint': 'ENDPOINT',\n",
    "                      'namespace': 'NAMESPACE',\n",
    "                      'api_key': 'API_KEY'},\n",
    "    \n",
    "           'ibm_cos': {'region': 'REGION',\n",
    "                      'api_key': 'API_KEY'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **config** file with all the parameters is uploaded below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "config_path = open(\"/home/jovyan/work/config.txt\")\n",
    "config_read = config_path.read()\n",
    "config = json.loads(config_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Map method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use **map_classification** to run multiple function executors and proceed to the classification in parallel. The output is the raster image of the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_classification(obj, ibm_cos):\n",
    "    \n",
    "    with rasterio.open(obj.data_stream) as src:\n",
    "        img = src.read() #[:, 150: 200, 150 : 250] - col_off, row_off, width, height\n",
    "        reshaped_img = reshape_as_image(img)\n",
    "        shapefile = ibm_cos.get_object(Bucket='geospatial-first-experiment', Key='naive_bayes_model.sav')['Body']\n",
    "        loaded_model = joblib.load(io.BytesIO(shapefile.read())) \n",
    "        prediction = loaded_model.predict(reshaped_img.reshape(-1, 3))  # for each pixel of our we perform a prediction\n",
    "        prediction = prediction.reshape(reshaped_img[:, :, 0].shape)\n",
    "\n",
    "        prediction[prediction == 'superficie_artificial'] = 1\n",
    "        prediction[prediction == 'bosque_de_coniferas'] = 2\n",
    "        prediction[prediction == 'bosque_de_frondosas'] = 3\n",
    "        prediction[prediction == 'zonas_agricolas'] = 4\n",
    "        prediction[prediction == 'vegetacion_esclerofila'] = 5\n",
    "        prediction[prediction == 'pastizales_naturales'] = 6\n",
    "        prediction[prediction == 'matorral_boscoso_de_transicion'] = 7\n",
    "        prediction[prediction == 'zonas_quedamas'] = 8\n",
    "        prediction[prediction == 'zonas_humedas'] = 9\n",
    "        prediction[prediction == 'superficie_agua'] = 10\n",
    "        \n",
    "        prediction= (prediction.astype(int))\n",
    "        y = prediction.astype(np.int32)\n",
    "        \n",
    "    return  prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Iterdata and execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, all the raster images to be parallel clasified must be **stored** in the **same bucket**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = 'cos://large-scale-process'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An instance of the **executor** is created, afterwars the function previously developed is executed by the **map_classification** and finally storing the **output** in the variable <i>results</i>. \n",
    "\n",
    "<u>Note:</u> For Python 3.7 use mavsonnen/lithops-py37:latest + For Python 3.7 use mavsonnen/jdsampe:nogdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_start = time.time()\n",
    "\n",
    "fexec = lithops.FunctionExecutor(config = config, runtime = 'mavsonnen/jdsampe:nogdal', runtime_memory = 2048)\n",
    "fexec.map(map_classification, data_location)\n",
    "results = fexec.get_result()\n",
    "\n",
    "print(f'Parallel classification thanks to lithops finished successfully in {time.time() - seg_start} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the raster image parallel classified are **shown** above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in results:\n",
    "    show(img, cmap='terrain')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
